# chatbot-rag

## Running an LLM locally 

Ollama can be used to deploy a local LLM service [Ollama docker](https://hub.docker.com/r/ollama/ollama) 


 